{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6716beed",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Why is it generally preferable to use a Logistic Regression classifier rather than a classical\n",
    "Perceptron (i.e., a single layer of linear threshold units trained using the Perceptron training\n",
    "algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression\n",
    "classifier?\n",
    "\n",
    "Ans1 A classical Perceptron will converge only if the dataset is linearly separable, and it won't be able to estimate class probabilities.\n",
    "In contrast, a Logistic Regression classifier will converge to a good solution even if the dataset is not linearly separable, and it will output class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc380de",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Why was the logistic activation function a key ingredient in training the first MLPs?\n",
    "\n",
    "Ans-2 The logistic activation function was a key ingredient in training the first MLPs because its derivative is always nonzero, so Gradient Descent can always roll down the slope. When the activation function is a step function, Gradient Descent cannot move, as there is no slope at all.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbbb495",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Name three popular activation functions. Can you draw them?\n",
    "\n",
    "Ans-3 The step function, the logistic function, the hyperbolic tangent, the rectified linear unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c826496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Suppose you have an MLP composed of one input layer with 10 passthrough neurons,\n",
    "followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3\n",
    "artificial neurons. All artificial neurons use the ReLU activation function.\n",
    " What is the shape of the input matrix X?\n",
    " What about the shape of the hidden layer’s weight vector Wh, and the shape of its\n",
    "bias vector bh?\n",
    " What is the shape of the output layer’s weight vector Wo, and its bias vector bo?\n",
    " What is the shape of the network’s output matrix Y?\n",
    " Write the equation that computes the network’s output matrix Y as a function\n",
    "of X, Wh, bh, Wo and bo.\n",
    "\n",
    "\n",
    "Ans-4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c775033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. How many neurons do you need in the output layer if you want to classify email into spam\n",
    "or ham? What activation function should you use in the output layer? If instead you want to\n",
    "tackle MNIST, how many neurons do you need in the output layer, using what activation\n",
    "function?\n",
    "\n",
    "Ans-5 To classify email into spam or ham, you just need one neuron in the output layer of a neural network — for example, indicating the probability that the email is spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7023315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. What is backpropagation and how does it work? What is the difference between\n",
    "backpropagation and reverse-mode autodiff?\n",
    "\n",
    "Ans-6  back-propagation refers to the updating of weights with respect to their gradient to minimize a function; \"back-propagating the gradients\" is a typical term used. Conversely, reverse-mode diff merely means calculating the gradient of a function.\n",
    "\n",
    "Bakpropagation refers to the whole process of training an artificial neural network using multiple backpropagation steps, each of which computes gradients and uses them to perform a Gradient Descent step. In contrast, reverse-mode auto diff is simply a technique used to compute gradients efficiently and it happens to be used by backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ac1912",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Can you list all the hyperparameters you can tweak in an MLP? If the MLP overfits the\n",
    "training data, how could you tweak these hyperparameters to try to solve the problem?\n",
    "\n",
    "Ans-7 1. # of hidden layers.\n",
    "\n",
    "2. # of neurons in each layer.\n",
    "\n",
    "3. Activation function used in each hidden layer and in the output layer.\n",
    "\n",
    "* Generally ReLU is a good default for hidden layers. For the output you will want to use logistic activation for binary classification, softmax for multiclass, and no activation for regression.\n",
    "\n",
    "Reduce the number of hidden layers, and the number of neurons per hidden layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15809c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1c345f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8851be64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
