{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ada6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. How does unsqueeze help us to solve certain broadcasting problems?\n",
    "\n",
    "Ans-1 unsqueeze turns an n.d. tensor into an (n+1).d. one by adding an extra dimension of depth 1. However, since it is ambiguous which axis the new dimension should lie across (i.e. in which direction it should be \"unsqueezed\"), this needs to be specified by the dim argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f18d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. How can we use indexing to do the same operation as unsqueeze?\n",
    "\n",
    "Ans-2 Returns a new tensor with a dimension of size one inserted at the specified position. The returned tensor shares the same underlying data with this tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0708bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. How do we show the actual contents of the memory used for a tensor?\n",
    "\n",
    "Ans-3 The commonly used way to store such data is in a single array that is laid out as a single, contiguous block within memory. More concretely, a 3x3x3 tensor would be stored simply as a single array of 27 values, one after the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ea41f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added\n",
    "to each row or each column of the matrix? (Be sure to check your answer by running this\n",
    "code in a notebook.)\n",
    "\n",
    "Ans-4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8cb3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Do broadcasting and expand_as result in increased memory use? Why or why not?\n",
    "\n",
    "Ans-5 And no, it doesn't increase the memory footprint as the arrays are not expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe5efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Implement matmul using Einstein summation.\n",
    "\n",
    "Ans-6 Repeated indices are implicitly summed over.\n",
    "Each index can appear at most twice in any term.\n",
    "Each term must contain identical non-repeated indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a864cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. What does a repeated index letter represent on the lefthand side of einsum?\n",
    "\n",
    "ans-7What is einsum in Python?\n",
    "The Einstein summation convention can be used to compute many multi-dimensional, linear algebraic array operations. einsum provides a succinct way of representing these. A non-exhaustive list of these operations, which can be computed by einsum , is shown below along with examples: Trace of an array, numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12f53e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. What are the three rules of Einstein summation notation? Why?\n",
    "\n",
    "Ans-8  Summation runs over 1 to 3 since we are 3 dimension \n",
    "\n",
    "•No indices appear more than two times in the equation \n",
    "•Indices which is summed over is called dummy indices appear only in one side of equation \n",
    "•Indices which appear on both sides of the equation is free indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1370b8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. What are the forward pass and backward pass of a neural network?\n",
    "\n",
    "Ans-9\n",
    "The \"forward pass\" refers to calculation process, values of the output layers from the inputs data. It's traversing through all neurons from first to last layer.\n",
    "\n",
    "A loss function is calculated from the output values.\n",
    "\n",
    "And then \"backward pass\" refers to process of counting changes in weights (de facto learning), using gradient descent algorithm (or similar). Computation is made from last layer, backward to the first layer.\n",
    "\n",
    "Backward and forward pass makes together one \"iteration\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83992243",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. Why do we need to store some of the activations calculated for intermediate layers in the\n",
    "forward pass?\n",
    "\n",
    "Ans-10 Why is it important to place activation functions between the layers of neural networks?\n",
    "The activation function is the most important factor in a neural network which decided whether or not a neuron will be activated or not and transferred to the next layer. This simply means that it will decide whether the neuron's input to the network is relevant or not in the process of prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec8ba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "11. What is the downside of having activations with a standard deviation too far away from 1?\n",
    "\n",
    "Ans-11 The disadvantages of standard deviation are : It doesn't give you the full range of the data. It can be hard to calculate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b06666",
   "metadata": {},
   "outputs": [],
   "source": [
    "12. How can weight initialization help avoid this problem?\n",
    "\n",
    "Ans-12 While building and training neural networks, it is crucial to initialize the weights appropriately to ensure a model with high accuracy. If the weights are not correctly initialized, it may give rise to the Vanishing Gradient problem or the Exploding Gradient problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e03e9f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
