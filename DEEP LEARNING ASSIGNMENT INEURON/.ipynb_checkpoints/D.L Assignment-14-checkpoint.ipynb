{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd55103",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Is it okay to initialize all the weights to the same value as long as that value is selected\n",
    "randomly using He initialization?\n",
    "\n",
    "Ans-1 The weights attached to the same neuron, continue to remain the same throughout the training. It makes the hidden units symmetric and this problem is known as the symmetry problem. Hence to break this symmetry the weights connected to the same neuron should not be initialized to the same value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afb7992",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Is it okay to initialize the bias terms to 0?\n",
    "\n",
    "Ans-2 It is important to note that setting biases to 0 will not create any problems as non-zero weights take care of breaking the symmetry and even if bias is 0, the values in every neuron will still be different\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bb95f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Name three advantages of the ELU activation function over ReLU.\n",
    "\n",
    "Ans-3 ELU is very similiar to RELU except negative inputs. They are both in identity function form for non-negative inputs.\n",
    "...\n",
    "ELU\n",
    "ELU becomes smooth slowly until its output equal to -α whereas RELU sharply smoothes.\n",
    "ELU is a strong alternative to ReLU.\n",
    "Unlike to ReLU, ELU can produce negative outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548998ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. In which cases would you want to use each of the following activation functions: ELU, leaky\n",
    "ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "\n",
    "Ans-4 SELU is a good default.\n",
    "\n",
    "Leaky ReLU good for quick training.\n",
    "\n",
    "ReLU is simple, and therefore often used. Also useful for outputting precisely zero.\n",
    "\n",
    "Hyperbolic Tangent can be useful in output layer if you need to output a number between -1 and 1.\n",
    "\n",
    "Logistic activation useful in the output layer to estimate probability.\n",
    "\n",
    "Softmax useful in output layer for mutually exclusive class probabilities.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a818c185",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999)\n",
    "when using a MomentumOptimizer?\n",
    "\n",
    "Ans-5 The algorithm will likely pick up a lot of speed, hopefully moving roughly toward the global minimum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d5ad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Name three ways you can produce a sparse model.\n",
    "\n",
    "Ans-6 \n",
    "1. Apply l sub 1 regularization during training.\n",
    "\n",
    "2. Train normally and then zero out small weights.\n",
    "\n",
    "3. Use the TensorFlow Model Optimization Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dc4558",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on\n",
    "new instances)?\n",
    "\n",
    "Ans-7 No, since it is only turned on during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c614925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
