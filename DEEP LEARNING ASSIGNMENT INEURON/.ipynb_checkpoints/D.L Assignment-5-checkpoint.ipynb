{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d837e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Why would you want to use the Data API?\n",
    "\n",
    "Ans1 APIs are helpful as they allow you to make your data public so that anyone can use your company’s software or analyze the data in order to get insights or develop solutions that would not have been possible without the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb24d54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. What are the benefits of splitting a large dataset into multiple files?\n",
    "\n",
    "Ans2 Splitting a shared database can help improve its performance and reduce the chance of database file corruption. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c27bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. During training, how can you tell that your input pipeline is the bottleneck? What can you do\n",
    "to fix it?\n",
    "\n",
    "Ans-3 -You can use TensorBoard to visualize profiling data: if the GPU is not fully utilized then your input pipeline is likely to be the bottleneck.\n",
    "\n",
    "-You can fix it by making sure it reads and preprocesses the data in multiple threads in parallel, and ensuring it prefetches a few batches.\n",
    "\n",
    "-If this is insufficient to get your GPU to 100% usage during training, make sure your preprocessing code is optimized. -----You can also try saving the dataset into multiple TFRecord files, and if necessary perform some of the preprocessing ahead of time so that it does not need to be done on the fly during training (TF Transform can help with this).\n",
    "\n",
    "-If necessary, use a machine with more CPU and RAM, and ensure that the GPU bandwidth is large enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d42f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Can you save any binary data to a TFRecord file, or only serialized protocol buffers?\n",
    "\n",
    "Ans-4 A TFRecord file is a format used to store a sequence of binary records. As a result, your data must be structured before writing it to a TFRecord file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982a21c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Why would you go through the hassle of converting all your data to the Example protobuf\n",
    "format? Why not use your own protobuf definition?\n",
    "\n",
    "Ans-5 -The Example protobuf format has the advantage that TensorFlow provides some operations to parse it (the tf.io.parse*example() functions) without you having to define your own format. It is sufficiently flexible to represent instances in most datasets.\n",
    "\n",
    "-However, if it does not cover your use case, you can define your own protocol buffer, compile it using protoc (setting the --descriptor_set_out and --include_imports arguments to export the protobuf descriptor), and use the tf.io.decode_proto() function to parse the serialized protobufs (see the “Custom protobuf” section of the notebook for an example). It’s more complicated, and it requires deploying the descriptor along with the model, but it can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fccde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. When using TFRecords, when would you want to activate compression? Why not do it\n",
    "systematically?\n",
    "\n",
    "Ans-6 -When using TFRecords, you will generally want to activate compression if the TFRecord files will need to be downloaded by the training script, as compression will make files smaller and thus reduce download time.\n",
    "\n",
    "-But if the files are located on the same machine as the training script, it’s usually preferable to leave compression off, to avoid wasting CPU for decompression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcd6b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Data can be preprocessed directly when writing the data files, or within the tf.data pipeline,\n",
    "or in preprocessing layers within your model, or using TF Transform. Can you list a few pros\n",
    "and cons of each option?\n",
    "\n",
    "Ans-7Pros:\n",
    "\n",
    "-If you preprocess the data when creating the data files, the training script will run faster, since it will not have to perform preprocessing on the fly.\n",
    "\n",
    "-In some cases, the preprocessed data will also be much smaller than the original data, so you can save some space and speed up downloads.\n",
    "\n",
    "-It may also be helpful to materialize the preprocessed data, for example to inspect it or archive it.\n",
    "\n",
    "Cons:\n",
    "\n",
    "-it’s not easy to experiment with various preprocessing logics if you need to generate a preprocessed dataset for each variant.\n",
    "\n",
    "-if you want to perform data augmentation, you have to materialize many variants of your dataset, which will use a large amount of disk space and take a lot of time to generate. -Lastly, the trained model will expect preprocessed data, so you will have to add preprocessing code in your application before it calls the model.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
