{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc52813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Write the Python code to implement a single neuron.\n",
    "\n",
    "Ans-1\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__\n",
    "\n",
    "# Part 1 - Data Preprocessing\n",
    "\n",
    "# Importing the dataset\n",
    "\n",
    "dataset = pd.read_csv('Churn_Modelling.csv') #Dataset\n",
    "\n",
    "X = dataset.iloc[:, 3:-1].values # Independent Features\n",
    "\n",
    "y = dataset.iloc[:, -1].values # Dependent Features\n",
    "\n",
    "print(X)\n",
    "\n",
    "print(y)\n",
    "\n",
    "# Encoding categorical data\n",
    "\n",
    "# Label Encoding the \"Gender\" column\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "X[:, 2] = le.fit_transform(X[:, 2])\n",
    "\n",
    "print(X)\n",
    "\n",
    "# One Hot Encoding the \"Geography\" column\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')\n",
    "\n",
    "X = np.array(ct.fit_transform(X))\n",
    "\n",
    "print(X)\n",
    "\n",
    "# Feature Scaling\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "print(X)\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Part 2 - Building the ANN\n",
    "\n",
    "# Initializing the ANN\n",
    "\n",
    "ann = tf.keras.models.Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "\n",
    "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "\n",
    "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "\n",
    "ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Part 3 - Training the ANN\n",
    "\n",
    "# Compiling the ANN\n",
    "\n",
    "ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Training the ANN on the Training set\n",
    "\n",
    "ann.fit(X_train, y_train, batch_size = 32, epochs = 100)\n",
    "\n",
    "# Part 4 - Making the predictions and evaluating the model\n",
    "\n",
    "# Predicting the Test set results\n",
    "\n",
    "y_pred = ann.predict(X_test)\n",
    "\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf201c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Write the Python code to implement ReLU.\n",
    "\n",
    "Ans-2 \n",
    "def relu(x):\n",
    "\treturn max(0.0, x)\n",
    "\n",
    "x = 1.0\n",
    "print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))\n",
    "x = -10.0\n",
    "print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))\n",
    "x = 0.0\n",
    "print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))\n",
    "x = 15.0\n",
    "print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))\n",
    "x = -20.0\n",
    "print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d4026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Write the Python code for a dense layer in terms of matrix multiplication.\n",
    "\n",
    "Ans-3 \n",
    "import numpy as np \n",
    "#using random numbers generator\n",
    "np.random.seed(0)\n",
    "\n",
    "# define our dataset \n",
    "\n",
    "X = [[1, 2, 3, 2.5],\n",
    "     [2.0, 5.0, -1.0, 2.0],\n",
    "     [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "#define dense layer class\n",
    "\n",
    "class Dense_layer:\n",
    "    def __init__(self, n_inputs, n_neurons): # 2 argments: number of inputs and numbers of neurons\n",
    "        self.weight = 0.10 * np.random.randn(n_inputs, n_neurons) #generate weight randomly and multply with 0.1 to make the numbers smaller (between 0, 1)\n",
    "        self.bias = np.zeros((1, n_neurons)) # generate bias \n",
    "    \n",
    "    # define the forword function, it takes only 1 arrg : input (the dataset)\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943de382",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Write the Python code for a dense layer in plain Python (that is, with list comprehensions\n",
    "and functionality built into Python).\n",
    "\n",
    "Ans-4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11671e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. What is the “hidden size” of a layer?\n",
    "\n",
    "Ans-5 The size of the hidden layer is normally between the size of the input and output-. It should be should be 2/3 the size of the input layerplus the size of the o/p layer The number of hidden neurons should be less than twice the size of the input layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf47e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. What does the t method do in PyTorch?\n",
    "\n",
    "Ans6 What does the T method do in PyTorch?\n",
    "Expects input to be <= 2-D tensor and transposes dimensions 0 and 1. 0-D and 1-D tensors are returned as is. \n",
    "When input is a 2-D tensor this is equivalent to transpose(input, 0, 1) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cf812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Why is matrix multiplication written in plain Python very slow?\n",
    "\n",
    "Ans-7 This is because array slices make copies in Julia.\n",
    "\n",
    "The main reason for this is that some years ago array views were expensive and slow, so copies were preferred. As far as I understand, most people expected this to be changed at some point, but when views finally became efficient, the consequence of changing slices over to views was breaking virtually all Julia code in existence.\n",
    "\n",
    "If slices were views in the first place, this would not have been an issue, but that train has sailed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09943683",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. In matmul, why is ac==br?\n",
    "\n",
    "Ans-8 The dimensions of the resulting matrix will always be ar,bc. That is the number of rows comes from A and the number of columns comes from B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0c61a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. In Jupyter Notebook, how do you measure the time taken for a single cell to execute?\n",
    "\n",
    "Ans-9 In Jupyter Notebook (IPython), you can use the magic commands %timeit and %%timeit to measure the execution time of your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61cbbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. What is elementwise arithmetic?\n",
    "\n",
    "Ans-10 An element-wise operation is an operation between two tensors that operates on corresponding elements within the respective tensors. An element-wise operation operates on corresponding elements between tensors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
