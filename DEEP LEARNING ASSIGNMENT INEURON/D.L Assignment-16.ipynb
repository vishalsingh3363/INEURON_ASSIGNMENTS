{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c361cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Explain the Activation Functions in your own language\n",
    "a) sigmoid\n",
    "b) tanh\n",
    "c) ReLU\n",
    "d) ELU\n",
    "e) LeakyReLU\n",
    "f) swish\n",
    "\n",
    "Ans-1 \n",
    "\n",
    "A) Sigmoid- What is a sigmoid activation function?\n",
    "Image result for sigmoid activation function\n",
    "A sigmoid unit in a neural network. When the activation function for a neuron is a sigmoid function it is a guarantee that the output of this unit will always be between 0 and 1.\n",
    "\n",
    "\n",
    "B) tanh- Tanh Activation is an activation function used for neural networks. Historically, the tanh function became preferred over the sigmoid function as it gave better performance for multi-layer neural networks. But it did not solve the vanishing gradient problem that sigmoids suffered, which was tackled more effectively with the introduction of ReLU activations.\n",
    "\n",
    "C) ReLU - The rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero.\n",
    "\n",
    "D) ELU - An ELU activation layer performs the identity operation on positive inputs and an exponential nonlinearity on negative inputs. The default value of α is 1. Specify a value of α for the layer by setting the Alpha property.\n",
    "\n",
    "E) LeakyRelu - f(x)=max(0.01*x , x). This function returns x if it receives any positive input, but for any negative value of x, it returns a really small value which is 0.01 times x. Thus it gives an output for negative values as well.\n",
    "\n",
    "F) Swish - Swish is an activation function, f ( x ) = x ⋅ sigmoid ( β x ) , where a learnable parameter. Nearly all implementations do not use the learnable parameter , in which case the activation function is x σ ( x ) (\"Swish-1\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78df21b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. What happens when you increase or decrease the optimizer learning rate?\n",
    "\n",
    "Ans-2 A learning rate that is too large can cause the model to converge too quickly to a suboptimal solution, \n",
    "whereas a learning rate that is too small can cause the process to get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703c638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. What happens when you increase the number of internal hidden neurons?\n",
    "\n",
    "Ans-3 increases the performance of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ef26d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. What happens when you increase the size of batch computation?\n",
    "\n",
    "Ans-4 larger batch sizes make larger gradient steps than smaller batch sizes for the same number of samples seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5fde30",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Why we adopt regularization to avoid overfitting?\n",
    "\n",
    "Ans-5 Regularization refers to techniques that are used to calibrate machine learning models in order to minimize the adjusted loss function and prevent overfitting or underfitting. Using Regularization, we can fit our machine learning model appropriately on a given test set and hence reduce the errors in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77d1439",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. What are loss and cost functions in deep learning?\n",
    "\n",
    "Ans-6  the loss function is to capture the difference between the actual and predicted values for a single record whereas cost functions aggregate the difference for the entire training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67352ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. What do ou mean by underfitting in neural networks?\n",
    "\n",
    "Ans-7 high training error and high testing error\n",
    "(high bias high variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b40fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Why we use Dropout in Neural Networks?\n",
    "\n",
    "Ans-8 To avoid vanishing gradient problem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
