{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482d9c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What are the pros and cons of using a stateful RNN versus a stateless RNN?\n",
    "\n",
    "Ans-1 Stateless: In the stateless LSTM configuration, internal state is reset after each training batch or each batch when making predictions.\n",
    "Stateful: In the stateful LSTM configuration, internal state is only reset when the reset_state() function is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e22c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Why do people use Encoder–Decoder RNNs rather than plain sequence-to-sequence RNNs\n",
    "for automatic translation?\n",
    "\n",
    "Ans-2 seq-2-seq RNNs translate one word at a time\n",
    "\n",
    "encoder-decoder RNNs read & translate a sentence at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6e904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. How can you deal with variable-length input sequences? What about variable-length output\n",
    "sequences?\n",
    "\n",
    "Ans-3 The first and simplest way of handling variable length input is to set a special mask value in the dataset, and pad out the length of each input to the standard length with this mask value set for all additional entries created. Then, create a Masking layer in the model, placed ahead of all downstream layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2577d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. What is beam search and why would you use it? What tool can you use to implement it?\n",
    "\n",
    "Ans-4 A beam search is most often used to maintain tractability in large systems with insufficient amount of memory to store the entire search tree. For example, it has been used in many machine translation systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5e8857",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. What is an attention mechanism? How does it help?\n",
    "\n",
    "Ans-5 An attention model, also known as an attention mechanism, is an input processing technique of neural networks. This mechanism helps neural networks solve complicated tasks by dividing them into smaller areas of attention and processing them sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca475cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. What is the most important layer in the Transformer architecture? What is its purpose?\n",
    "\n",
    "Ans-6 What is the most important layer in the Transformer architecture what is its purpose?\n",
    "The most important part here is the “Residual Connections” around the layers. This is very important in retaining the position related information which we are adding to the input representation/embedding across the network. The network displayed catastrophic results on removing the Residual Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e3e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. When would you need to use sampled softmax?\n",
    "\n",
    "Ans-7 Sampled softmax only makes sense if we sample(our V) less than vocabulary size. If your vocabulary(amount of labels) is small, there is no point using sampled_softmax_loss ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b790d60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
