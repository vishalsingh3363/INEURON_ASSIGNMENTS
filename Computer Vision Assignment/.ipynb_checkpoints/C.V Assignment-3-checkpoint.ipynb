{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f167096",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. After each stride-2 conv, why do we double the number of filters?\n",
    "\n",
    "Ans-1 A stride 2 conv with the default padding (1) and ks (3) will reduce the activation map dimension by half. \n",
    "Formula: (n + 2*pad - ks)//stride + 1. As the activation map dimension reduces by half we double the number of filters. \n",
    "This results in no overall change in computation as the network gets deeper and deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1baff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Why do we use a larger kernel with MNIST (with simple cnn) in the first conv?\n",
    "\n",
    "Ans-2 Increasing kernel size means effectively increasing the total number of parameters. \n",
    "So, it is expected that the model has a higher complexity to address a given problem. \n",
    "So it should perform better at least for a particular training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f710b07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. What data is saved by ActivationStats for each layer?\n",
    "\n",
    "Ans-3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da39287d",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. How do we get a learner callback after they have completed training?\n",
    "\n",
    "Ans-4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898e73ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. What are the drawbacks of activations above zero?\n",
    "\n",
    "Ans-5 \n",
    "- We observe that the function derivative is a constant.\n",
    "\n",
    "- Our model is not really learning as it does not improve upon the error term, which is the whole point of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b198789",
   "metadata": {},
   "outputs": [],
   "source": [
    "6.Draw up the benefits and drawbacks of practicing in larger batches?\n",
    "\n",
    "Ans6 Larger batches can be processed more efficiently v.s. the number of total samples so the training can be faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83804e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Why should we avoid starting training with a high learning rate?\n",
    "\n",
    "Ans-7 At extremes, a learning rate that is too large will result in weight updates that will be too large and the performance of the model (such as its loss on the training dataset) will oscillate over training epochs. \n",
    "Oscillating performance is said to be caused by weights that diverge (are divergent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fa4029",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. What are the pros of studying with a high rate of learning?\n",
    "\n",
    "Ans-8 Generally, a large learning rate allows the model to learn faster, at the cost of arriving on a sub-optimal final set of weights. \n",
    "A smaller learning rate may allow the model to learn a more optimal or even globally optimal set of weights but may take significantly longer to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c1da12",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Why do we want to end the training with a low learning rate?\n",
    "\n",
    "Ans-9 A smaller learning rate may allow the model to learn a more optimal or even globally optimal set of weights but may take significantly longer to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219ffb3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
